{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Data - #Zeroplastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Amma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import inflect\n",
    "import re\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import json \n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed Successfully\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"dataset/\" directory.\n",
    "# Read the input raw data and parse only the necessary columns from the json file.\n",
    "from datetime import datetime\n",
    "def populate_tweet_df(tweets):\n",
    "    \"\"\"\n",
    "    This function takes tweets list as argument and returns a dataframe of the tweets.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['date'] = [datetime.strptime(tweet['created_at'],'%a %b %d %H:%M:%S %z %Y') for tweet in tweets if (tweet['lang'] == 'en')]\n",
    "    df['text'] = [tweet['text'] for tweet in tweets if (tweet['lang'] == 'en')]\n",
    "    # df['text'] = list(map(lambda tweet:tweet['text'], filter(lambda tweet: tweet['lang']=='en', tweets)))\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tweet_file1 = (\"tweets_zeroplastic.txt\")\n",
    "    tweets = []\n",
    "    with open(tweet_file1, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            tweets.append(json.loads(line))\n",
    "    \n",
    "    tweets_df = populate_tweet_df(tweets)\n",
    "    print(\"Executed Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-09 14:21:05+00:00</td>\n",
       "      <td>üí¨‚ÄúThere is no such thing as ‚Äòaway‚Äô. When we th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-09 13:42:32+00:00</td>\n",
       "      <td>Ditch the plastic. Custom tote bags with desig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-09 13:15:54+00:00</td>\n",
       "      <td>Itsu's solution to plastic.....\\n\\nIt feels be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-09 13:14:12+00:00</td>\n",
       "      <td>RT @EarthbasicsU: All our products are üçÉüçÇ #bio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-09 11:16:25+00:00</td>\n",
       "      <td>RT @EarthbasicsU: All our products are üçÉüçÇ #bio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date                                               text\n",
       "0 2020-03-09 14:21:05+00:00  üí¨‚ÄúThere is no such thing as ‚Äòaway‚Äô. When we th...\n",
       "1 2020-03-09 13:42:32+00:00  Ditch the plastic. Custom tote bags with desig...\n",
       "2 2020-03-09 13:15:54+00:00  Itsu's solution to plastic.....\\n\\nIt feels be...\n",
       "3 2020-03-09 13:14:12+00:00  RT @EarthbasicsU: All our products are üçÉüçÇ #bio...\n",
       "4 2020-03-09 11:16:25+00:00  RT @EarthbasicsU: All our products are üçÉüçÇ #bio..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dictionary for Contractions\n",
    "contractions_dict = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "     \n",
    "#expanding contractions. e.g \"don't\" to \"do not\"    \n",
    "def contractions(text, contraction_mapping=contractions_dict):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "# Define a dictioary of emoticons\n",
    "emoticons_dictionary = { ':-)'  : b'\\xf0\\x9f\\x98\\x8a',\n",
    "                      ':)'   : b'\\xf0\\x9f\\x98\\x8a',\n",
    "                      '=)'   : b'\\xf0\\x9f\\x98\\x8a',  # Smile or happy\n",
    "                     ':-D'  : b'\\xf0\\x9f\\x98\\x83',\n",
    "                      ':D'   : b'\\xf0\\x9f\\x98\\x83',\n",
    "                      '=D'   : b'\\xf0\\x9f\\x98\\x83',  # Big smile\n",
    "                      '>:-(' : b'\\xF0\\x9F\\x98\\xA0',\n",
    "                      '>:-o' : b'\\xF0\\x9F\\x98\\xA0'   # Angry face\n",
    "                      }\n",
    "#Function converts emoticons to emoji\n",
    "def convert_emoticons(emoticons):\n",
    "    emoticons=emoticons.replace('.',' ')\n",
    "    emoticons=emoticons.replace(',',' ')\n",
    "    for i in emoticons.split():\n",
    "        if i in emoticons_dictionary.keys():\n",
    "            word=emoticons_dictionary[i].decode('utf-8')\n",
    "            emoticons=emoticons.replace(i,word)\n",
    "    return emoticons\n",
    "\n",
    "#Function to convert emoji to word\n",
    "def convert_emoji_to_word(emo_converted_text):\n",
    "    for i in emo_converted_text:\n",
    "        if i in emoji.UNICODE_EMOJI:\n",
    "            emo_word=str(emoji.demojize(i))\n",
    "            rep_colon=emo_word.replace(':',' ')\n",
    "            rep_dash=rep_colon.replace('_',' ')\n",
    "            emo_converted_text=emo_converted_text.replace(i,rep_dash)\n",
    "    return emo_converted_text\n",
    "\n",
    "#preprocessing the text\n",
    "def clean_text(text):\n",
    "    #remove links,username,other hashtags and accronyms\n",
    "    text=str(text)\n",
    "    text=re.sub(r\"http\\S+\", \"\", text)\n",
    "    text=re.sub(r\"#\\S+\", \"\",text)\n",
    "    text=re.sub(r\"@\\S+\", \"\",text)\n",
    "    text = re.sub(r\"\\b[A-Z]{2,}\\b\", \"\", text)\n",
    "    \n",
    "    emoticons_treated=convert_emoticons(text)\n",
    "    text=convert_emoji_to_word(emoticons_treated)\n",
    "    \n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    \n",
    "    #remove contractions\n",
    "    text = contractions(text)\n",
    "    \n",
    "    # Tokenize the article: tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Convert the tokens into lowercase: lower_tokens\n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    #Convert digits to words\n",
    "    p = inflect.engine()\n",
    "    alpha_num_tokens = [p.number_to_words(t) if t.isdigit() else t for t in lower_tokens]\n",
    "    \n",
    "    #Remove all punctuations\n",
    "    alpha_tokens = [t for t in alpha_num_tokens if t.isalpha()]\n",
    "    \n",
    "    #Remove all stop words except no and not\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.remove('not')\n",
    "    stop_words.remove('no')\n",
    "    no_stops = [t for t in alpha_tokens if t not in stop_words]\n",
    "    \n",
    "    return no_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def get_hypernyms(tweet):\n",
    "\n",
    "    new_tweet = tweet\n",
    "\n",
    "    # We create a counter to iterate through each word\n",
    "    counter = 0\n",
    "    # Now we iterate through each word to get the\n",
    "    for w in tweet:\n",
    "        synset = wordnet.synsets(w)\n",
    "\n",
    "        # We check the length of the synset; we continue only if we get a response from wordnet.synsets\n",
    "        if len(synset) > 0:\n",
    "            # We get the hypernym\n",
    "            hyper = synset[0].hypernyms()\n",
    "            # If we get a result\n",
    "            if len(hyper) > 0:\n",
    "                first_lemma = hyper[0]\n",
    "                lemmas = first_lemma.lemma_names()\n",
    "                new_tweet[counter] = lemmas[0]\n",
    "\n",
    "        # We increase the counter\n",
    "        counter += 1\n",
    "\n",
    "    return new_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_score(s):\n",
    "    # We split the tweet by spaces to get each individual word.\n",
    "    split_text_o = s.split()\n",
    "    split_text_uniq = set(split_text_o)\n",
    "    split_text = list(split_text_uniq)\n",
    "\n",
    "    # We start by assigning 0 to the sentiment_score, this sentiment_score will consist of the sum of all\n",
    "    # sentiment scores for each individual word.\n",
    "    sentiment_score = 0  # sentiment_score\n",
    "\n",
    "    # Here we specify the denominator that we will use to get the average\n",
    "    # We will only take into account the words that have a score in wordnet.synsets\n",
    "    syn_denominator = 0\n",
    "    # We specify the overall score\n",
    "    overall_score = 0\n",
    "\n",
    "    # Now we replace each for in the tweet for its hypernym\n",
    "    split_text = get_hypernyms(split_text)\n",
    "\n",
    "    # Now we iterate through each word to get the\n",
    "    for w in split_text:\n",
    "        synset = wordnet.synsets(w)\n",
    "        # We check the length of the synset; we continue only if we get a response from wordnet.synsets\n",
    "        if len(synset) > 0:\n",
    "            # synset[0].name() contains the id that we will use to reference the word later in senti_synset\n",
    "            # We assign this value to the variable name\n",
    "            name = synset[0].name()\n",
    "            # Below we get the positive and negative scores for the word using its name as id\n",
    "            breakdown = swn.senti_synset(name)\n",
    "            # We get the negative and positive scores\n",
    "            pos_score = breakdown.pos_score()\n",
    "            neg_score = breakdown.neg_score()\n",
    "\n",
    "            # Then we calculate the sentiment_score for this word and add it up to the scores of the previous words.\n",
    "            sentiment_score += pos_score - neg_score  # The sentiment_score of all words in the tweet.\n",
    "\n",
    "            # We increase the syn_denominator count + 1\n",
    "            syn_denominator += 1\n",
    "    # Now we will calculate the average using syn_denominator as denominator\n",
    "    if syn_denominator != 0:\n",
    "        # Now we calculate the average and assign it to the overall_score\n",
    "        # The overall_score will be the mean of each score\n",
    "        overall_score = sentiment_score / syn_denominator\n",
    "\n",
    "    # We return the tweet's overall_score.\n",
    "    return overall_score,split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we iterate through all the files and assign a score\n",
    "def assign_scores(files_list):\n",
    "    neg=0\n",
    "    pos=0\n",
    "    neu=0\n",
    "    for file in files_list:\n",
    "        # All files are txt files.\n",
    "        #f = open('Datasets/' + file + '.txt')\n",
    "        f = open(file + '.txt')\n",
    "        # We read the lines of the txt file.\n",
    "        lines = f.readlines()\n",
    "\n",
    "        # We create the sentences dictionary. This will contain an array of sentences which each element will contain the\n",
    "        # text and an overall score for the whole sentence.\n",
    "        sentences = []\n",
    "\n",
    "        # Counter for each sentence in the corpus, this is for later comparison with the file where we extracted the data.\n",
    "        count = 0\n",
    "\n",
    "        # For each line in the file we will proceed to\n",
    "        for line in lines:\n",
    "            # Data Extraction\n",
    "            # We parse each fine of the file.\n",
    "            parsed_json = (json.loads(line))\n",
    "\n",
    "            # Discriminate to only take english sentences\n",
    "            if parsed_json['lang'] == 'en':\n",
    "                # We extract the text from the parsed line\n",
    "                text = parsed_json['text']\n",
    "                created_at = parsed_json['created_at']\n",
    "                # Now we proceed to clean the text\n",
    "                cleaned_set = clean_text(text)\n",
    "                after_clean_text = \" \".join(cleaned_set)\n",
    "\n",
    "                # Now we iterate and we take the hypernyms of each word\n",
    "                # We get the score for each sentence\n",
    "                score,hypernyms = get_tweet_score(after_clean_text)\n",
    "                hypernyms = \" \".join(hypernyms)\n",
    "                if score > 0:\n",
    "                    neg+=1\n",
    "                elif score == 0:\n",
    "                    neu+=1\n",
    "                else:\n",
    "                    pos+=1\n",
    "                # Dictionary creation\n",
    "                # We create a dictionary for the sentence\n",
    "                sentences.append({\n",
    "                    'created_at': created_at,\n",
    "                    'text': text,\n",
    "                    'after_clean_text': after_clean_text,\n",
    "                    'hypernyms': hypernyms,\n",
    "                    'score': score\n",
    "                })\n",
    "                # We increase the counter\n",
    "            count += 1\n",
    "\n",
    "        # We create a new file for each hash tag file that we consulted.\n",
    "        with open(file + '_final_' + '.json', 'w') as outfile:\n",
    "            # We finally dump the tweets + the overall_score in a json file.\n",
    "            json.dump(sentences, outfile, indent=2)\n",
    "\n",
    "        # End of the program's execution\n",
    "        print(\"Total lines = \" + str(count) + \" we're processed.\")\n",
    "        print(\"Score assignation has finished successfully!\") \n",
    "        #Plot the number of positive,negative and neutral scores\n",
    "        import matplotlib.pyplot as plt\n",
    "        x_cord=[2,4,6]\n",
    "        y_cord=[neg,neu,pos]\n",
    "        tick_label=['Negative','Neutral','Positive']\n",
    "        plt.bar(x_cord, y_cord, tick_label = tick_label, \n",
    "        width = 0.8, color = ['blue','orange', 'green']) \n",
    "        # naming the x-axis \n",
    "        plt.xlabel('Polarity') \n",
    "        # naming the y-axis \n",
    "        plt.ylabel('Count') \n",
    "        # plot title \n",
    "        plt.title('Representation of various Sentiments') \n",
    "  \n",
    "        # function to show the plot \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines = 345 we're processed.\n",
      "Score assignation has finished successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from nltk.corpus import wordnet  #Import wordnet from the NLTK\n",
    "from nltk.corpus import sentiwordnet as swn # We import sentiwordnet\n",
    "files_list = ['tweets_zeroplastic']\n",
    "assign_scores(files_list)\n",
    "\n",
    "# We load the url of the first file.\n",
    "url = files_list[0] + '_final_' + \".json\"\n",
    "\n",
    "# We will open the file\n",
    "with open(url, 'r') as json_file:\n",
    "    data = json_file.read()\n",
    "\n",
    "# Now we parse the file\n",
    "parsed = json.loads(data)\n",
    "\n",
    "# Pretty Printing JSON string back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines = 345 we're processed.\n",
      "Score assignation has finished successfully!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_files_list = ['tweets_zeroplastic']\n",
    "assign_scores(new_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': 'Mon Mar 09 14:21:05 +0000 2020', 'text': 'üí¨‚ÄúThere is no such thing as ‚Äòaway‚Äô. When we throw anything away it must go somewhere.‚Äù ‚Äì Annie Leonard‚Ä¶ https://t.co/6Vn5xfPbN1', 'after_clean_text': 'speech balloon no thing away throw anything away must go somewhere annie', 'hypernyms': 'away shift negative location propulsion speech_act situation necessity annie lighter-than-air_craft anything', 'score': -0.027777777777777776}\n",
      "\n",
      "{'created_at': 'Mon Mar 09 13:42:32 +0000 2020', 'text': 'Ditch the plastic. Custom tote bags with designs to inspire #kindness at https://t.co/J1siEx1zOd. 100% #organic cot‚Ä¶ https://t.co/zD62LlMJUm', 'after_clean_text': 'ditch plastic custom tote bags designs inspire', 'hypernyms': 'solid bag stimulate container excavation practice creating_by_mental_acts', 'score': 0.017857142857142856}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We load the url of the first file.\n",
    "url = new_files_list[0] + \"_final_\" + \".json\"\n",
    "\n",
    "# JSON file\n",
    "f = open (url, \"r\")\n",
    "\n",
    "# Reading from file\n",
    "data = json.loads(f.read())\n",
    "\n",
    "for i in range(2):\n",
    "\tprint(str(data[i])+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the word embedding file\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_embeddings(file_name):\n",
    "    \n",
    "    #dict to store word embeddings from glove file\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join('glove.6B', 'glove.6B.100d.txt'), encoding='utf8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    print('Found %s word vectors in glove dict' % len(embeddings_index))\n",
    "    #generate a set of unique tokens in entire corpus\n",
    "    reviews = pd.read_csv(file_name)\n",
    "    unique_tokens = set()\n",
    "    for review in reviews['text']:\n",
    "        unique_tokens.update(clean_text(review))\n",
    "        \n",
    "    print(\"Found %s unique tokens in corpus\" % len(unique_tokens))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create a dictionary of tokens and their embeddings\n",
    "    tokens_embeddings_dict = {}\n",
    "    \n",
    "    for word in unique_tokens:\n",
    "        word_vector = embeddings_index.get(word)\n",
    "        if word_vector is not None:\n",
    "            tokens_embeddings_dict[word] = word_vector\n",
    "            \n",
    "    print(\"Number of embeddings from corpus generated: %s\" % len(tokens_embeddings_dict))\n",
    "    \n",
    "    #Generate dataframe from dictionary to export\n",
    "    # Create dataframe from dic and make keys, index in dataframe\n",
    "    embeddings_df = pd.DataFrame.from_dict(tokens_embeddings_dict, orient='index')\n",
    "    embeddings_df.to_csv(\"project_embeddings.csv\")\n",
    "    \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn, sentiwordnet as swn\n",
    "# Instantiate the WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors in glove dict\n",
      "Found 575 unique tokens in corpus\n",
      "Number of embeddings from corpus generated: 568\n"
     ]
    }
   ],
   "source": [
    "#generate embeddings for our corpus, makes a file project_embeddings.csv and stores it in directory\n",
    "generate_embeddings(\"tweets with hypernyms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.read_csv(\"project_embeddings.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering\n",
    "\n",
    "def cluster_embeddings(df, num_of_clusters):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = num_of_clusters, random_state=42).fit(df)\n",
    "    group_num = kmeans.labels_\n",
    "    geo_centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    #assign nearest word to geometric centroid in embedding space as centroid\n",
    "    # find the index of the closest points from x to each class centroid\n",
    "    close = pairwise_distances_argmin_min(geo_centroids, df, metric='euclidean')\n",
    "    index_closest_points = close[0]\n",
    "    word_centroids = df.iloc[index_closest_points].index\n",
    "    \n",
    "    #create dict of group number and centroids\n",
    "    centroid_dict = {}\n",
    "    for i in range(len(index_closest_points)):\n",
    "        centroid_dict[i] = word_centroids[i]\n",
    "    \n",
    "    #create a dictionary of word and corresponding centroid\n",
    "    \n",
    "    #replace each label(group number) assigned by kmeans cluster algo with centroid word\n",
    "    cen = [centroid_dict.get(group) for group in group_num]\n",
    "\n",
    "    #create a dictionary\n",
    "    word_centroid_dict = {}\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        word_centroid_dict[df.index[i]] = cen[i]\n",
    "    \n",
    "    return word_centroid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_centroids(text, word_centroid_dict):\n",
    "    \n",
    "    new_text = [word_centroid_dict.get(word) if word in word_centroid_dict else word for word in text]\n",
    "    \n",
    "    return new_text   \n",
    "    \n",
    "    \n",
    "\n",
    "def swn_classifier(text):\n",
    " \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0.0\n",
    "    \n",
    "    #Calculating score\n",
    "    for word in text:\n",
    "        \n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if not lemma:\n",
    "            continue\n",
    " \n",
    "        synsets = wn.synsets(lemma)\n",
    "        if not synsets:\n",
    "            continue\n",
    "        \n",
    "        # Take the first synset, the most common\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "        print()\n",
    " \n",
    "        #sentiment is the difference between positive and negative score\n",
    "        sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "        tokens_count += 1\n",
    " \n",
    "    # Default: neither positive, nor negative\n",
    "    if not tokens_count:\n",
    "        return 0\n",
    " \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the cluster function to generate a word centroid dictionary\n",
    "num = 55   #roughly 11 words per cluster\n",
    "word_centroid_dict = cluster_embeddings(embeddings_df, num)\n",
    "#n_clusters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"tweets with hypernyms.csv\")\n",
    "\n",
    "#arrays to store list of tokens, replaced words, scores\n",
    "clean_tokens = []\n",
    "replaced_tokens = []\n",
    "y_predicted = []\n",
    "\n",
    "\n",
    "#calling the main calculate function\n",
    "for text in tweets[\"text\"]:    \n",
    "    clean_t = clean_text(text)\n",
    "    clean_tokens.append(clean_t)\n",
    "    \n",
    "    replaced_t = replace_with_centroids(clean_t, word_centroid_dict)\n",
    "    replaced_tokens.append(replaced_t)\n",
    "    \n",
    "    senti_score = swn_classifier(replaced_t)\n",
    "    y_predicted.append(senti_score)\n",
    "    \n",
    "#Classify reviews according to setiment score assigned\n",
    "#1 : positive, 0 : neutral, -1 : negative \n",
    "y_classified = []\n",
    "for i in y_predicted:\n",
    "    if i > 0:\n",
    "        y_classified.append(1)\n",
    "    elif i<0:\n",
    "        y_classified.append(-1)\n",
    "    elif i==0:\n",
    "        y_classified.append(0)\n",
    "        \n",
    "\n",
    "#appending cols in df\n",
    "#tweets[\"hypernyms\"]\n",
    "tweets[\"tokens\"] = clean_tokens\n",
    "tweets[\"replaced_centroids\"] = replaced_tokens\n",
    "tweets[\"sentiment_score\"] = y_predicted\n",
    "tweets[\"predicted_sentiment\"] = y_classified\n",
    "\n",
    "#exporting df\n",
    "tweets.to_csv(\"Score_generation_hypernyms_word_embedding.csv\", header=True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>after_clean_text</th>\n",
       "      <th>hypernyms</th>\n",
       "      <th>score</th>\n",
       "      <th>tokens</th>\n",
       "      <th>replaced_centroids</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Mar 09 14:21:05 +0000 2020</td>\n",
       "      <td>\\ud83d\\udcac\\u201cThere is no such thing as \\u...</td>\n",
       "      <td>speech balloon no thing away throw anything aw...</td>\n",
       "      <td>location shift propulsion lighter-than-air_cra...</td>\n",
       "      <td>-0.027778</td>\n",
       "      <td>[no, thing, throw, anything, away, must, go, s...</td>\n",
       "      <td>[not, nothing, come, nothing, come, not, come,...</td>\n",
       "      <td>-1.375</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mon Mar 09 13:42:32 +0000 2020</td>\n",
       "      <td>Ditch the plastic. Custom tote bags with desig...</td>\n",
       "      <td>ditch plastic custom tote bags designs inspire</td>\n",
       "      <td>bag container stimulate excavation practice cr...</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>[ditch, plastic, custom, tote, bags, designs, ...</td>\n",
       "      <td>[aim, stainless, use, tote, tote, products, hope]</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Mar 09 13:15:54 +0000 2020</td>\n",
       "      <td>Itsu's solution to plastic.....\\n\\nIt feels be...</td>\n",
       "      <td>itsus solution plastic feels better nothing se...</td>\n",
       "      <td>good be awareness not advantage sensing itsus ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>[itsus, solution, plastic, feels, better, noth...</td>\n",
       "      <td>[itsus, aim, stainless, nothing, way, nothing,...</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mon Mar 09 13:14:12 +0000 2020</td>\n",
       "      <td>RT @EarthbasicsU: All our products are \\ud83c\\...</td>\n",
       "      <td>products leaf fluttering wind fallen leaf herb...</td>\n",
       "      <td>vascular_plant external_body_part use weather ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[products]</td>\n",
       "      <td>[products]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mon Mar 09 11:16:25 +0000 2020</td>\n",
       "      <td>RT @EarthbasicsU: All our products are \\ud83c\\...</td>\n",
       "      <td>products leaf fluttering wind fallen leaf herb...</td>\n",
       "      <td>vascular_plant external_body_part use weather ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[products]</td>\n",
       "      <td>[products]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Mon Mar 09 14:21:05 +0000 2020   \n",
       "1  Mon Mar 09 13:42:32 +0000 2020   \n",
       "2  Mon Mar 09 13:15:54 +0000 2020   \n",
       "3  Mon Mar 09 13:14:12 +0000 2020   \n",
       "4  Mon Mar 09 11:16:25 +0000 2020   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\ud83d\\udcac\\u201cThere is no such thing as \\u...   \n",
       "1  Ditch the plastic. Custom tote bags with desig...   \n",
       "2  Itsu's solution to plastic.....\\n\\nIt feels be...   \n",
       "3  RT @EarthbasicsU: All our products are \\ud83c\\...   \n",
       "4  RT @EarthbasicsU: All our products are \\ud83c\\...   \n",
       "\n",
       "                                    after_clean_text  \\\n",
       "0  speech balloon no thing away throw anything aw...   \n",
       "1     ditch plastic custom tote bags designs inspire   \n",
       "2  itsus solution plastic feels better nothing se...   \n",
       "3  products leaf fluttering wind fallen leaf herb...   \n",
       "4  products leaf fluttering wind fallen leaf herb...   \n",
       "\n",
       "                                           hypernyms     score  \\\n",
       "0  location shift propulsion lighter-than-air_cra... -0.027778   \n",
       "1  bag container stimulate excavation practice cr...  0.017857   \n",
       "2  good be awareness not advantage sensing itsus ...  0.100000   \n",
       "3  vascular_plant external_body_part use weather ...  0.000000   \n",
       "4  vascular_plant external_body_part use weather ...  0.000000   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [no, thing, throw, anything, away, must, go, s...   \n",
       "1  [ditch, plastic, custom, tote, bags, designs, ...   \n",
       "2  [itsus, solution, plastic, feels, better, noth...   \n",
       "3                                         [products]   \n",
       "4                                         [products]   \n",
       "\n",
       "                                  replaced_centroids  sentiment_score  \\\n",
       "0  [not, nothing, come, nothing, come, not, come,...           -1.375   \n",
       "1  [aim, stainless, use, tote, tote, products, hope]            0.125   \n",
       "2  [itsus, aim, stainless, nothing, way, nothing,...           -0.625   \n",
       "3                                         [products]            0.000   \n",
       "4                                         [products]            0.000   \n",
       "\n",
       "   predicted_sentiment  \n",
       "0                   -1  \n",
       "1                    1  \n",
       "2                   -1  \n",
       "3                    0  \n",
       "4                    0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \\ud83d\\udcac\\u201cThere is no such thing as \\u2018away\\u2019. When we throw anything away it must go somewhere.\\u201d \\u2013 Annie Leonard\\u2026 https://t.co/6Vn5xfPbN1\n",
      "tokens: ['no', 'thing', 'throw', 'anything', 'away', 'must', 'go', 'somewhere', 'annie']\n",
      "replaced centroids: ['not', 'nothing', 'come', 'nothing', 'come', 'not', 'come', 'nothing', 'madness']\n",
      "sentiment_score: -1.375 \n",
      "\n",
      "text: RT @EarthbasicsU: All our products are \\ud83c\\udf43\\ud83c\\udf42 #biodegradable or\\ud83c\\udf3f#vegan or #crueltyfree \\ud83d\\udc30 or #sustainablysourced or\\u267b\\ufe0f #recyclable or #zeroplasti\\u2026\n",
      "tokens: ['products']\n",
      "replaced centroids: ['products']\n",
      "sentiment_score: 0.0 \n",
      "\n",
      "text: RT @EarthbasicsU: All our products are \\ud83c\\udf43\\ud83c\\udf42 #biodegradable or\\ud83c\\udf3f#vegan or #crueltyfree \\ud83d\\udc30 or #sustainablysourced or\\u267b\\ufe0f #recyclable or #zeroplasti\\u2026\n",
      "tokens: ['products']\n",
      "replaced centroids: ['products']\n",
      "sentiment_score: 0.0 \n",
      "\n",
      "text: RT @PulpTecLtd: Our mission is an open book : we aim to reduce our carbon footprint \\u2013 100% recyclable, biodegradable and compostable!\n",
      "\n",
      "#Wor\\u2026\n",
      "tokens: ['mission', 'open', 'book', 'aim', 'reduce', 'carbon', 'footprint', 'recyclable', 'biodegradable', 'compostable']\n",
      "replaced centroids: ['aim', 'instead', 'read', 'aim', 'reducing', 'reducing', 'multitude', 'biodegradable', 'biodegradable', 'biodegradable']\n",
      "sentiment_score: 1.875 \n",
      "\n",
      "text: RT @annie_knowsbest: Food lovers of Twitter, are there any GREAT #zerowaste stores in your neck of the woods? I mean offering over and abov\\u2026\n",
      "tokens: ['food', 'lovers', 'twitter', 'stores', 'neck', 'woods', 'mean', 'offering']\n",
      "replaced centroids: ['eat', 'madness', 'twitter', 'store', 'hand', 'green', 'nothing', 'providing']\n",
      "sentiment_score: 0.375 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Some examples:\n",
    "for n in range(0,100,20):\n",
    "    \n",
    "    print(\"text:\",tweets[\"text\"][n])\n",
    "    print(\"tokens:\",tweets[\"tokens\"][n])\n",
    "    print(\"replaced centroids:\",tweets[\"replaced_centroids\"][n])\n",
    "    print(\"sentiment_score:\",tweets[\"sentiment_score\"][n],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annie': 'lovely',\n",
       " 'hands': 'instead',\n",
       " 'mexico': 'states',\n",
       " 'staff': 'given',\n",
       " 'walk': 'instead',\n",
       " 'making': 'time',\n",
       " 'alternative': 'innovative',\n",
       " 'pounds': 'pounds',\n",
       " 'planned': 'next',\n",
       " 'take': 'come',\n",
       " 'roll': 'instead',\n",
       " 'people': 'come',\n",
       " 'recent': 'recent',\n",
       " 'welsh': 'wales',\n",
       " 'transport': 'instead',\n",
       " 'effects': 'affecting',\n",
       " 'eight': 'five',\n",
       " 'team': 'time',\n",
       " 'reusable': 'compostable',\n",
       " 'average': 'drop',\n",
       " 'wales': 'wales',\n",
       " 'nothing': 'nothing',\n",
       " 'reuse': 'compostable',\n",
       " 'customer': 'offering',\n",
       " 'interesting': 'fantastic',\n",
       " 'bags': 'bag',\n",
       " 'hear': 'tell',\n",
       " 'step': 'follow',\n",
       " 'ability': 'given',\n",
       " 'flour': 'ingredients',\n",
       " 'cruelty': 'madness',\n",
       " 'growing': 'recent',\n",
       " 'bar': 'instead',\n",
       " 'line': 'instead',\n",
       " 'lost': 'time',\n",
       " 'lifestyle': 'lifestyles',\n",
       " 'rs': 'heres',\n",
       " 'still': 'not',\n",
       " 'wash': 'shampoo',\n",
       " 'setup': 'replaces',\n",
       " 'swan': 'lovely',\n",
       " 'must': 'not',\n",
       " 'given': 'given',\n",
       " 'bill': 'calls',\n",
       " 'blend': 'products',\n",
       " 'virus': 'virus',\n",
       " 'big': 'time',\n",
       " 'available': 'given',\n",
       " 'world': 'time',\n",
       " 'trash': 'bag',\n",
       " 'compostable': 'compostable',\n",
       " 'three': 'five',\n",
       " 'trip': 'next',\n",
       " 'address': 'calls',\n",
       " 'range': 'given',\n",
       " 'bulk': 'instead',\n",
       " 'nations': 'states',\n",
       " 'seems': 'nothing',\n",
       " 'dennis': 'replaces',\n",
       " 'whole': 'nothing',\n",
       " 'stomachs': 'nourish',\n",
       " 'reality': 'mind',\n",
       " 'sustainable': 'innovative',\n",
       " 'harmful': 'affecting',\n",
       " 'version': 'shows',\n",
       " 'diapers': 'shampoo',\n",
       " 'meets': 'mind',\n",
       " 'snowing': 'heres',\n",
       " 'friends': 'tell',\n",
       " 'shower': 'shampoo',\n",
       " 'eat': 'tell',\n",
       " 'dish': 'ingredients',\n",
       " 'level': 'given',\n",
       " 'plastics': 'stainless',\n",
       " 'rely': 'instead',\n",
       " 'passed': 'calls',\n",
       " 'good': 'nothing',\n",
       " 'applause': 'calls',\n",
       " 'repost': 'heres',\n",
       " 'toothbrushes': 'shampoo',\n",
       " 'work': 'come',\n",
       " 'businesses': 'offering',\n",
       " 'weekend': 'next',\n",
       " 'hand': 'instead',\n",
       " 'plastic': 'bag',\n",
       " 'ways': 'follow',\n",
       " 'spring': 'next',\n",
       " 'states': 'states',\n",
       " 'favor': 'calls',\n",
       " 'giant': 'small',\n",
       " 'done': 'nothing',\n",
       " 'crystal': 'fantastic',\n",
       " 'zero': 'drop',\n",
       " 'fat': 'products',\n",
       " 'serving': 'instead',\n",
       " 'beautiful': 'lovely',\n",
       " 'gently': 'bag',\n",
       " 'exceptional': 'given',\n",
       " 'taught': 'mind',\n",
       " 'long': 'time',\n",
       " 'sized': 'small',\n",
       " 'drawing': 'instead',\n",
       " 'cloth': 'bag',\n",
       " 'bristle': 'heres',\n",
       " 'instead': 'instead',\n",
       " 'brushes': 'shampoo',\n",
       " 'going': 'come',\n",
       " 'ensuring': 'follow',\n",
       " 'year': 'next',\n",
       " 'marine': 'found',\n",
       " 'protect': 'follow',\n",
       " 'designs': 'innovative',\n",
       " 'fair': 'nothing',\n",
       " 'thank': 'tell',\n",
       " 'ethical': 'mind',\n",
       " 'comes': 'mind',\n",
       " 'lot': 'nothing',\n",
       " 'behind': 'time',\n",
       " 'materials': 'stainless',\n",
       " 'say': 'come',\n",
       " 'link': 'given',\n",
       " 'ingredients': 'products',\n",
       " 'remind': 'tell',\n",
       " 'billion': 'billion',\n",
       " 'warehouse': 'store',\n",
       " 'look': 'come',\n",
       " 'solution': 'follow',\n",
       " 'sometimes': 'sometimes',\n",
       " 'collect': 'instead',\n",
       " 'peek': 'heres',\n",
       " 'oceans': 'oceans',\n",
       " 'folk': 'shows',\n",
       " 'tooth': 'neck',\n",
       " 'raises': 'drop',\n",
       " 'different': 'sometimes',\n",
       " 'record': 'time',\n",
       " 'higher': 'drop',\n",
       " 'sanitary': 'compostable',\n",
       " 'set': 'next',\n",
       " 'replace': 'replaces',\n",
       " 'comment': 'post',\n",
       " 'food': 'ingredients',\n",
       " 'woods': 'mind',\n",
       " 'time': 'time',\n",
       " 'switch': 'instead',\n",
       " 'anything': 'nothing',\n",
       " 'merchants': 'offering',\n",
       " 'reduce': 'drop',\n",
       " 'way': 'come',\n",
       " 'might': 'not',\n",
       " 'dolphins': 'galapagos',\n",
       " 'wrapped': 'bag',\n",
       " 'veg': 'heres',\n",
       " 'swap': 'instead',\n",
       " 'sale': 'offering',\n",
       " 'lovely': 'lovely',\n",
       " 'tell': 'tell',\n",
       " 'convenient': 'cheaper',\n",
       " 'recyclable': 'compostable',\n",
       " 'think': 'nothing',\n",
       " 'come': 'come',\n",
       " 'enforce': 'follow',\n",
       " 'blank': 'instead',\n",
       " 'want': 'come',\n",
       " 'united': 'states',\n",
       " 'fact': 'nothing',\n",
       " 'came': 'next',\n",
       " 'worlds': 'fantastic',\n",
       " 'trees': 'small',\n",
       " 'initiative': 'calls',\n",
       " 'five': 'five',\n",
       " 'madness': 'madness',\n",
       " 'email': 'info',\n",
       " 'mums': 'heres',\n",
       " 'planet': 'mind',\n",
       " 'learning': 'given',\n",
       " 'manufacturing': 'products',\n",
       " 'embark': 'follow',\n",
       " 'multitude': 'heres',\n",
       " 'learn': 'tell',\n",
       " 'ugly': 'lovely',\n",
       " 'weigh': 'instead',\n",
       " 'extra': 'instead',\n",
       " 'stainless': 'stainless',\n",
       " 'like': 'come',\n",
       " 'cleaning': 'shampoo',\n",
       " 'lasts': 'mind',\n",
       " 'refills': 'compostable',\n",
       " 'cotton': 'products',\n",
       " 'cole': 'replaces',\n",
       " 'news': 'post',\n",
       " 'keep': 'come',\n",
       " 'free': 'instead',\n",
       " 'international': 'recent',\n",
       " 'complete': 'given',\n",
       " 'vegan': 'lifestyles',\n",
       " 'goes': 'mind',\n",
       " 'single': 'time',\n",
       " 'voted': 'calls',\n",
       " 'convenience': 'store',\n",
       " 'bees': 'bees',\n",
       " 'earlier': 'next',\n",
       " 'straw': 'bag',\n",
       " 'pattern': 'sometimes',\n",
       " 'feel': 'nothing',\n",
       " 'clean': 'instead',\n",
       " 'could': 'not',\n",
       " 'mark': 'post',\n",
       " 'support': 'calls',\n",
       " 'evening': 'next',\n",
       " 'local': 'home',\n",
       " 'surge': 'drop',\n",
       " 'city': 'home',\n",
       " 'castle': 'home',\n",
       " 'project': 'innovative',\n",
       " 'candles': 'candles',\n",
       " 'earth': 'mind',\n",
       " 'produced': 'shows',\n",
       " 'shine': 'fantastic',\n",
       " 'face': 'time',\n",
       " 'bathrooms': 'shampoo',\n",
       " 'practising': 'heres',\n",
       " 'considering': 'follow',\n",
       " 'cheaper': 'cheaper',\n",
       " 'stores': 'store',\n",
       " 'made': 'time',\n",
       " 'ever': 'time',\n",
       " 'replaces': 'replaces',\n",
       " 'looking': 'come',\n",
       " 'everywhere': 'tell',\n",
       " 'girl': 'lovely',\n",
       " 'stats': 'info',\n",
       " 'nobody': 'nothing',\n",
       " 'let': 'come',\n",
       " 'tips': 'instead',\n",
       " 'uhh': 'heres',\n",
       " 'form': 'given',\n",
       " 'years': 'five',\n",
       " 'adopted': 'calls',\n",
       " 'mind': 'mind',\n",
       " 'logic': 'mind',\n",
       " 'book': 'post',\n",
       " 'contributed': 'recent',\n",
       " 'us': 'come',\n",
       " 'change': 'follow',\n",
       " 'totally': 'nothing',\n",
       " 'working': 'come',\n",
       " 'except': 'instead',\n",
       " 'weather': 'oceans',\n",
       " 'source': 'given',\n",
       " 'visiting': 'home',\n",
       " 'back': 'time',\n",
       " 'open': 'next',\n",
       " 'tap': 'instead',\n",
       " 'every': 'time',\n",
       " 'six': 'five',\n",
       " 'creatively': 'nourish',\n",
       " 'looks': 'mind',\n",
       " 'amp': 'replaces',\n",
       " 'first': 'next',\n",
       " 'concern': 'recent',\n",
       " 'proud': 'tell',\n",
       " 'pollutes': 'nourish',\n",
       " 'twitter': 'info',\n",
       " 'posted': 'post',\n",
       " 'water': 'ingredients',\n",
       " 'eco': 'heres',\n",
       " 'etsy': 'heres',\n",
       " 'hold': 'come',\n",
       " 'ciara': 'heres',\n",
       " 'innovative': 'innovative',\n",
       " 'washes': 'shampoo',\n",
       " 'rolling': 'instead',\n",
       " 'guest': 'shows',\n",
       " 'soap': 'shampoo',\n",
       " 'outweigh': 'nourish',\n",
       " 'kindness': 'madness',\n",
       " 'works': 'shows',\n",
       " 'click': 'info',\n",
       " 'anyone': 'nothing',\n",
       " 'organic': 'products',\n",
       " 'offering': 'offering',\n",
       " 'current': 'recent',\n",
       " 'facts': 'mind',\n",
       " 'major': 'recent',\n",
       " 'use': 'given',\n",
       " 'simple': 'sometimes',\n",
       " 'togetherness': 'madness',\n",
       " 'monday': 'next',\n",
       " 'lifestyles': 'lifestyles',\n",
       " 'street': 'market',\n",
       " 'fill': 'instead',\n",
       " 'bottles': 'shampoo',\n",
       " 'japanese': 'recent',\n",
       " 'little': 'time',\n",
       " 'mean': 'nothing',\n",
       " 'bad': 'nothing',\n",
       " 'thanks': 'time',\n",
       " 'eradicating': 'nourish',\n",
       " 'cents': 'cents',\n",
       " 'oils': 'products',\n",
       " 'tiffin': 'heres',\n",
       " 'confirmed': 'found',\n",
       " 'times': 'post',\n",
       " 'government': 'calls',\n",
       " 'consumption': 'products',\n",
       " 'lovers': 'lovely',\n",
       " 'insects': 'bees',\n",
       " 'found': 'found',\n",
       " 'cut': 'drop',\n",
       " 'conditioner': 'shampoo',\n",
       " 'perfect': 'fantastic',\n",
       " 'bulky': 'compostable',\n",
       " 'small': 'small',\n",
       " 'read': 'tell',\n",
       " 'skin': 'neck',\n",
       " 'neck': 'neck',\n",
       " 'smile': 'fantastic',\n",
       " 'two': 'five',\n",
       " 'economic': 'recent',\n",
       " 'either': 'instead',\n",
       " 'may': 'not',\n",
       " 'see': 'come',\n",
       " 'past': 'time',\n",
       " 'nourish': 'nourish',\n",
       " 'hour': 'next',\n",
       " 'unique': 'innovative',\n",
       " 'increased': 'drop',\n",
       " 'everybody': 'nothing',\n",
       " 'much': 'time',\n",
       " 'always': 'nothing',\n",
       " 'better': 'come',\n",
       " 'bravos': 'heres',\n",
       " 'fish': 'found',\n",
       " 'cool': 'mind',\n",
       " 'thing': 'nothing',\n",
       " 'know': 'nothing',\n",
       " 'mission': 'follow',\n",
       " 'least': 'five',\n",
       " 'antarctica': 'galapagos',\n",
       " 'blows': 'throw',\n",
       " 'regulate': 'affecting',\n",
       " 'pandemic': 'virus',\n",
       " 'next': 'next',\n",
       " 'steel': 'stainless',\n",
       " 'thin': 'bag',\n",
       " 'ban': 'calls',\n",
       " 'treat': 'sometimes',\n",
       " 'smallest': 'small',\n",
       " 'bio': 'heres',\n",
       " 'switzerland': 'states',\n",
       " 'towards': 'follow',\n",
       " 'great': 'time',\n",
       " 'cost': 'drop',\n",
       " 'items': 'products',\n",
       " 'new': 'next',\n",
       " 'closing': 'next',\n",
       " 'thousands': 'instead',\n",
       " 'seven': 'five',\n",
       " 'difference': 'given',\n",
       " 'wee': 'heres',\n",
       " 'away': 'time',\n",
       " 'aim': 'follow',\n",
       " 'eggs': 'bees',\n",
       " 'surprises': 'fantastic',\n",
       " 'deliver': 'follow',\n",
       " 'totes': 'heres',\n",
       " 'cradle': 'madness',\n",
       " 'dispenser': 'shampoo',\n",
       " 'continent': 'oceans',\n",
       " 'image': 'mind',\n",
       " 'bag': 'bag',\n",
       " 'impressed': 'fantastic',\n",
       " 'journey': 'mind',\n",
       " 'days': 'next',\n",
       " 'thumbs': 'heres',\n",
       " 'baby': 'lovely',\n",
       " 'everyone': 'nothing',\n",
       " 'research': 'recent',\n",
       " 'month': 'next',\n",
       " 'quote': 'info',\n",
       " 'woodley': 'heres',\n",
       " 'life': 'mind',\n",
       " 'tote': 'bag',\n",
       " 'not': 'not',\n",
       " 'body': 'found',\n",
       " 'goal': 'time',\n",
       " 'please': 'tell',\n",
       " 'literally': 'sometimes',\n",
       " 'effort': 'follow',\n",
       " 'storms': 'oceans',\n",
       " 'incredible': 'fantastic',\n",
       " 'bought': 'offering',\n",
       " 'find': 'come',\n",
       " 'fantastic': 'fantastic',\n",
       " 'future': 'follow',\n",
       " 'selection': 'given',\n",
       " 'personalised': 'heres',\n",
       " 'biodegradable': 'compostable',\n",
       " 'amazing': 'fantastic',\n",
       " 'happy': 'tell',\n",
       " 'oats': 'ingredients',\n",
       " 'regular': 'next',\n",
       " 'village': 'home',\n",
       " 'without': 'time',\n",
       " 'info': 'info',\n",
       " 'hard': 'time',\n",
       " 'women': 'come',\n",
       " 'custom': 'sometimes',\n",
       " 'shopping': 'store',\n",
       " 'pump': 'instead',\n",
       " 'buy': 'offering',\n",
       " 'forum': 'calls',\n",
       " 'featuring': 'shows',\n",
       " 'market': 'market',\n",
       " 'together': 'time',\n",
       " 'colors': 'sometimes',\n",
       " 'mothers': 'tell',\n",
       " 'join': 'follow',\n",
       " 'home': 'home',\n",
       " 'within': 'given',\n",
       " 'caress': 'heres',\n",
       " 'viable': 'innovative',\n",
       " 'go': 'come',\n",
       " 'live': 'shows',\n",
       " 'ten': 'five',\n",
       " 'get': 'come',\n",
       " 'islands': 'galapagos',\n",
       " 'shoppers': 'offering',\n",
       " 'affecting': 'affecting',\n",
       " 'latest': 'recent',\n",
       " 'one': 'time',\n",
       " 'legislation': 'calls',\n",
       " 'something': 'nothing',\n",
       " 'huge': 'small',\n",
       " 'make': 'come',\n",
       " 'saving': 'instead',\n",
       " 'calls': 'calls',\n",
       " 'plans': 'follow',\n",
       " 'today': 'next',\n",
       " 'play': 'time',\n",
       " 'happening': 'nothing',\n",
       " 'shows': 'shows',\n",
       " 'gel': 'shampoo',\n",
       " 'habits': 'lifestyles',\n",
       " 'best': 'time',\n",
       " 'follow': 'follow',\n",
       " 'inspire': 'nourish',\n",
       " 'box': 'bag',\n",
       " 'problem': 'nothing',\n",
       " 'using': 'instead',\n",
       " 'got': 'time',\n",
       " 'buying': 'offering',\n",
       " 'toothbrush': 'shampoo',\n",
       " 'packaging': 'products',\n",
       " 'resting': 'neck',\n",
       " 'order': 'follow',\n",
       " 'improve': 'follow',\n",
       " 'opportunity': 'come',\n",
       " 'planted': 'found',\n",
       " 'drop': 'drop',\n",
       " 'galapagos': 'galapagos',\n",
       " 'bars': 'instead',\n",
       " 'store': 'store',\n",
       " 'shop': 'store',\n",
       " 'conscious': 'mind',\n",
       " 'believe': 'nothing',\n",
       " 'green': 'small',\n",
       " 'sparkling': 'lovely',\n",
       " 'video': 'shows',\n",
       " 'love': 'mind',\n",
       " 'retreat': 'follow',\n",
       " 'hope': 'come',\n",
       " 'invite': 'tell',\n",
       " 'decompose': 'nourish',\n",
       " 'post': 'post',\n",
       " 'everything': 'nothing',\n",
       " 'wow': 'fantastic',\n",
       " 'dump': 'bag',\n",
       " 'waste': 'pollution',\n",
       " 'carbon': 'pollution',\n",
       " 'house': 'home',\n",
       " 'another': 'time',\n",
       " 'door': 'instead',\n",
       " 'prepare': 'follow',\n",
       " 'start': 'next',\n",
       " 'liquid': 'stainless',\n",
       " 'action': 'calls',\n",
       " 'holding': 'instead',\n",
       " 'easter': 'celebrate',\n",
       " 'limited': 'given',\n",
       " 'footprint': 'globally',\n",
       " 'key': 'recent',\n",
       " 'heres': 'heres',\n",
       " 'beeswax': 'heres',\n",
       " 'fun': 'fantastic',\n",
       " 'spend': 'come',\n",
       " 'punch': 'throw',\n",
       " 'natural': 'given',\n",
       " 'depends': 'mind',\n",
       " 'recycled': 'compostable',\n",
       " 'give': 'come',\n",
       " 'handicrafts': 'compostable',\n",
       " 'smaller': 'small',\n",
       " 'onion': 'ingredients',\n",
       " 'delivery': 'instead',\n",
       " 'fulfillment': 'madness',\n",
       " 'delighted': 'fantastic',\n",
       " 'would': 'not',\n",
       " 'pioneering': 'innovative',\n",
       " 'paradise': 'madness',\n",
       " 'nutrition': 'ingredients',\n",
       " 'thinking': 'nothing',\n",
       " 'online': 'offering',\n",
       " 'countries': 'states',\n",
       " 'wrap': 'bag',\n",
       " 'scented': 'candles',\n",
       " 'rainbow': 'lovely',\n",
       " 'end': 'next',\n",
       " 'really': 'nothing',\n",
       " 'round': 'next',\n",
       " 'article': 'post',\n",
       " 'corona': 'heres',\n",
       " 'tons': 'pounds',\n",
       " 'day': 'next',\n",
       " 'gift': 'store',\n",
       " 'globally': 'globally',\n",
       " 'trade': 'market',\n",
       " 'half': 'five',\n",
       " 'million': 'billion',\n",
       " 'ensure': 'follow',\n",
       " 'technology': 'innovative',\n",
       " 'ditch': 'instead',\n",
       " 'wherever': 'tell',\n",
       " 'things': 'nothing',\n",
       " 'pinch': 'throw',\n",
       " 'makes': 'mind',\n",
       " 'stay': 'come',\n",
       " 'celebrate': 'celebrate',\n",
       " 'bamboo': 'small',\n",
       " 'shampoo': 'shampoo',\n",
       " 'providing': 'given',\n",
       " 'no': 'not',\n",
       " 'loads': 'instead',\n",
       " 'celebrating': 'celebrate',\n",
       " 'changes': 'follow',\n",
       " 'design': 'innovative',\n",
       " 'pollution': 'pollution',\n",
       " 'throw': 'throw',\n",
       " 'pig': 'found',\n",
       " 'week': 'next',\n",
       " 'around': 'time',\n",
       " 'giveaway': 'heres',\n",
       " 'reducing': 'affecting',\n",
       " 'booms': 'heres',\n",
       " 'leading': 'recent',\n",
       " 'speech': 'calls',\n",
       " 'less': 'time',\n",
       " 'products': 'products',\n",
       " 'yes': 'tell',\n",
       " 'feels': 'nothing',\n",
       " 'family': 'home',\n",
       " 'somewhere': 'mind',\n",
       " 'wildlife': 'oceans',\n",
       " 'replacing': 'replaces',\n",
       " 'wokingham': 'heres',\n",
       " 'hoodies': 'heres',\n",
       " 'affordable': 'cheaper'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_centroid_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
